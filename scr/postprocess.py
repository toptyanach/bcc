"""
postprocess.py - –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ OCR
–í–∫–ª—é—á–∞–µ—Ç LLM –æ–±—Ä–∞–±–æ—Ç–∫—É, –ø—Ä–∞–≤–∏–ª–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import re
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import requests
import os


def call_llm_to_json(raw_text: str, model_type: str = 'openai') -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    Args:
        raw_text: –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –æ—Ç OCR
        model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('openai', 'local', 'mock')

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
    """
    if not raw_text or not raw_text.strip():
        return {
            'error': '–ü—É—Å—Ç–æ–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç',
            'success': False
        }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–æ–π —Ç–∏–ø LLM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    if model_type == 'openai' and os.getenv('OPENAI_API_KEY'):
        return _call_openai_api(raw_text)
    elif model_type == 'local':
        return _call_local_llm(raw_text)
    else:
        # Fallback –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
        print("‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–∞-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")
        return _mock_llm_response(raw_text)


def _call_openai_api(text: str) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ OpenAI API –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    try:
        import openai

        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

        prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.
        –ù–∞–π–¥–∏ –∏ –≤–µ—Ä–Ω–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å:
        - fio: –ø–æ–ª–Ω–æ–µ –∏–º—è —á–µ–ª–æ–≤–µ–∫–∞
        - date: –ª—é–±—ã–µ –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
        - sum: –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –∫–∞–∫ —á–∏—Å–ª–∞
        - contract_number: –Ω–æ–º–µ—Ä–∞ –¥–æ–≥–æ–≤–æ—Ä–æ–≤/–∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
        - phone: –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        - email: –∞–¥—Ä–µ—Å–∞ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç—ã
        - inn: –ò–ù–ù (10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä)
        - address: –∞–¥—Ä–µ—Å–∞
        - doc_type: —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–æ–≥–æ–≤–æ—Ä, —Å—á–µ—Ç, –∑–∞—è–≤–ª–µ–Ω–∏–µ –∏ —Ç.–¥.)

        –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
        {text}

        –û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        """

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0
        )

        result_text = response.choices[0].message.content

        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            result = json.loads(result_text)
            result['llm_processed'] = True
            result['llm_model'] = 'openai'
            result['success'] = True
            return result
        except json.JSONDecodeError:
            # –ï—Å–ª–∏ JSON –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return {
                'raw_llm_response': result_text,
                'llm_processed': True,
                'llm_model': 'openai',
                'success': True,
                'error': '–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –æ—Ç LLM'
            }

    except Exception as e:
        return {
            'error': f'–û—à–∏–±–∫–∞ OpenAI API: {str(e)}',
            'success': False,
            'llm_processed': False
        }


def _call_local_llm(text: str) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM –º–æ–¥–µ–ª–∏"""
    try:
        # –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (Ollama, LocalAI –∏ —Ç.–¥.)
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥ –¥–ª—è –≤–∞—à–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏

        # –ü—Ä–∏–º–µ—Ä –¥–ª—è Ollama
        response = requests.post('http://localhost:11434/api/generate', json={
            'model': 'llama2',  # –∏–ª–∏ –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
            'prompt': f'''
            Extract structured data from this document text as JSON:

            {text}

            Return JSON with fields: fio, date, sum, phone, email, inn, doc_type
            ''',
            'stream': False
        }, timeout=30)

        if response.status_code == 200:
            result = response.json()
            llm_text = result.get('response', '')

            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            json_match = re.search(r'\{.*\}', llm_text, re.DOTALL)
            if json_match:
                try:
                    parsed = json.loads(json_match.group())
                    parsed['llm_processed'] = True
                    parsed['llm_model'] = 'local'
                    parsed['success'] = True
                    return parsed
                except json.JSONDecodeError:
                    pass

            return {
                'raw_llm_response': llm_text,
                'llm_processed': True,
                'llm_model': 'local',
                'success': True
            }

    except Exception as e:
        return {
            'error': f'–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π LLM: {str(e)}',
            'success': False,
            'llm_processed': False
        }


def _mock_llm_response(text: str) -> Dict[str, Any]:
    """–ò–º–∏—Ç–∞—Ü–∏—è LLM –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ø—Ä–∞–≤–∏–ª"""
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    result = {
        'llm_processed': False,
        'llm_model': 'mock_rules',
        'success': True
    }

    # –î–∞—Ç—ã
    date_pattern = r'\b(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})\b'
    date_matches = re.findall(date_pattern, text)
    if date_matches:
        try:
            day, month, year = date_matches[0]
            date_obj = datetime(int(year), int(month), int(day))
            result['date'] = date_obj.strftime('%Y-%m-%d')
        except ValueError:
            pass

    # –°—É–º–º—ã
    sum_pattern = r'(\d{1,3}(?:[\s,]\d{3})*(?:[.,]\d{1,2})?)\s*(?:—Ä—É–±|—Ä—É–±–ª–µ–π|—Ä\.|‚ÇΩ)'
    sum_matches = re.findall(sum_pattern, text, re.IGNORECASE)
    if sum_matches:
        try:
            sum_str = sum_matches[0].replace(' ', '').replace(',', '.')
            result['sum'] = float(sum_str)
        except ValueError:
            pass

    # –§–ò–û (–ø—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ)
    fio_pattern = r'\b([–ê-–Ø–Å][–∞-—è—ë]+)\s+([–ê-–Ø–Å][–∞-—è—ë]+)(?:\s+([–ê-–Ø–Å][–∞-—è—ë]+))?\b'
    fio_matches = re.findall(fio_pattern, text)
    if fio_matches:
        fio_parts = [part for part in fio_matches[0] if part]
        result['fio'] = ' '.join(fio_parts)

    # –¢–µ–ª–µ—Ñ–æ–Ω
    phone_pattern = r'(?:\+7|8|7)?[\s\-]?\(?\d{3}\)?[\s\-]?\d{3}[\s\-]?\d{2}[\s\-]?\d{2}'
    phone_matches = re.findall(phone_pattern, text)
    if phone_matches:
        phone = re.sub(r'[^\d+]', '', phone_matches[0])
        if len(phone) >= 10:
            result['phone'] = phone

    # Email
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    email_matches = re.findall(email_pattern, text, re.IGNORECASE)
    if email_matches:
        result['email'] = email_matches[0].lower()

    # –ò–ù–ù
    inn_pattern = r'\b\d{10}\b|\b\d{12}\b'
    inn_matches = re.findall(inn_pattern, text)
    if inn_matches:
        result['inn'] = inn_matches[0]

    # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
    doc_keywords = {
        '–¥–æ–≥–æ–≤–æ—Ä': ['–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ'],
        '—Å—á–µ—Ç': ['—Å—á–µ—Ç', 'invoice', '—Ñ–∞–∫—Ç—É—Ä–∞'],
        '–∑–∞—è–≤–ª–µ–Ω–∏–µ': ['–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–∑–∞—è–≤–∫–∞'],
        '–∞–∫—Ç': ['–∞–∫—Ç'],
        '—Å–ø—Ä–∞–≤–∫–∞': ['—Å–ø—Ä–∞–≤–∫–∞', '–≤—ã–ø–∏—Å–∫–∞']
    }

    text_lower = text.lower()
    for doc_type, keywords in doc_keywords.items():
        if any(keyword in text_lower for keyword in keywords):
            result['doc_type'] = doc_type
            break

    return result


def rules_based_postprocess(extracted_data: Dict, raw_text: str) -> Dict[str, Any]:
    """
    –ü—Ä–∞–≤–∏–ª–∞-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

    Args:
        extracted_data: –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∏–∑ OCR
        raw_text: –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –û—á–∏—â–µ–Ω–Ω—ã–µ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    result = extracted_data.copy() if extracted_data else {}

    # –û—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    result = _clean_and_validate_fields(result)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ raw_text –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø—Ä–æ–ø—É—â–µ–Ω–æ
    result = _extract_missing_fields(result, raw_text)

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª
    result = _apply_business_rules(result)

    result['postprocessed'] = True
    result['postprocess_method'] = 'rules_based'

    return result


def _clean_and_validate_fields(data: Dict) -> Dict[str, Any]:
    """–û—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–µ–π"""
    cleaned = {}

    for field, value in data.items():
        if value is None or value == '':
            continue

        # –û—á–∏—Å—Ç–∫–∞ –§–ò–û
        if field == 'fio' and isinstance(value, str):
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            fio_clean = ' '.join(word.capitalize() for word in value.split() if word.isalpha())
            if len(fio_clean.split()) >= 2:  # –ú–∏–Ω–∏–º—É–º –∏–º—è –∏ —Ñ–∞–º–∏–ª–∏—è
                cleaned[field] = fio_clean

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        elif field == 'phone' and isinstance(value, str):
            phone_clean = re.sub(r'[^\d+]', '', value)
            if len(phone_clean) >= 10:
                # –ü—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É
                if phone_clean.startswith('8'):
                    phone_clean = '+7' + phone_clean[1:]
                elif phone_clean.startswith('7'):
                    phone_clean = '+' + phone_clean
                elif not phone_clean.startswith('+'):
                    phone_clean = '+7' + phone_clean
                cleaned[field] = phone_clean

        # –í–∞–ª–∏–¥–∞—Ü–∏—è email
        elif field == 'email' and isinstance(value, str):
            email_pattern = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$'
            if re.match(email_pattern, value, re.IGNORECASE):
                cleaned[field] = value.lower()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ò–ù–ù
        elif field == 'inn' and isinstance(value, str):
            inn_clean = re.sub(r'[^\d]', '', value)
            if len(inn_clean) in [10, 12]:
                cleaned[field] = inn_clean

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç
        elif field == 'date' and isinstance(value, str):
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É
                if re.match(r'^\d{4}-\d{2}-\d{2}$', value):
                    datetime.strptime(value, '%Y-%m-%d')
                    cleaned[field] = value
                else:
                    # –ü—ã—Ç–∞–µ–º—Å—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º–∞—Ç—ã
                    normalized_date = _normalize_date_string(value)
                    if normalized_date:
                        cleaned[field] = normalized_date
            except ValueError:
                pass

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—É–º–º
        elif field == 'sum' and (isinstance(value, (int, float)) or isinstance(value, str)):
            try:
                if isinstance(value, str):
                    sum_clean = re.sub(r'[^\d.,]', '', value).replace(',', '.')
                    value = float(sum_clean)
                if value > 0:
                    cleaned[field] = value
            except ValueError:
                pass

        else:
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∫–æ–ø–∏—Ä—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
            cleaned[field] = value

    return cleaned


def _extract_missing_fields(data: Dict, raw_text: str) -> Dict[str, Any]:
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π"""
    result = data.copy()

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –∞–¥—Ä–µ—Å, –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å
    if 'address' not in result:
        address_keywords = ['–∞–¥—Ä–µ—Å', '–ø—Ä–æ–∂–∏–≤–∞–µ—Ç', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω', '–º–µ—Å—Ç–æ–Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ']
        for keyword in address_keywords:
            pattern = rf'{keyword}[:\s]+([^,\n]+(?:,[^,\n]+)*)'
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                result['address'] = match.group(1).strip()
                break

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if 'contract_number' not in result:
        # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ–º–µ—Ä–æ–≤
        number_patterns = [
            r'‚Ññ\s*(\d+(?:[-/]\d+)*)',
            r'–Ω–æ–º–µ—Ä[:\s]+(\d+(?:[-/]\d+)*)',
            r'–¥–æ–≥–æ–≤–æ—Ä[:\s]+‚Ññ?\s*(\d+(?:[-/]\d+)*)'
        ]

        for pattern in number_patterns:
            match = re.search(pattern, raw_text, re.IGNORECASE)
            if match:
                result['contract_number'] = match.group(1)
                break

    return result


def _apply_business_rules(data: Dict) -> Dict[str, Any]:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–∞–≤–∏–ª –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫"""
    result = data.copy()

    # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å –ò–ù–ù –∏ —Å—á–µ—Ç, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
    if result.get('inn') and result.get('account'):
        if 'doc_type' not in result:
            result['doc_type'] = '–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π_–¥–æ–∫—É–º–µ–Ω—Ç'

    # –ü—Ä–∞–≤–∏–ª–æ: –µ—Å–ª–∏ –µ—Å—Ç—å –±–æ–ª—å—à–∞—è —Å—É–º–º–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª—é—Ç—É
    if 'sum' in result and isinstance(result['sum'], (int, float)):
        if result['sum'] > 1000:
            result['currency'] = 'RUB'
        result['sum_formatted'] = f"{result['sum']:,.2f} ‚ÇΩ"

    # –ü—Ä–∞–≤–∏–ª–æ: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    if 'doc_type' in result:
        doc_type_mapping = {
            '–¥–æ–≥–æ–≤–æ—Ä': ['–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ'],
            '—Å—á–µ—Ç': ['—Å—á–µ—Ç', '—Å—á—ë—Ç', 'invoice', '—Ñ–∞–∫—Ç—É—Ä–∞'],
            '–∑–∞—è–≤–ª–µ–Ω–∏–µ': ['–∑–∞—è–≤–ª–µ–Ω–∏–µ', '–∑–∞—è–≤–∫–∞', '–æ–±—Ä–∞—â–µ–Ω–∏–µ'],
            '—Å–ø—Ä–∞–≤–∫–∞': ['—Å–ø—Ä–∞–≤–∫–∞', '–≤—ã–ø–∏—Å–∫–∞', '–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ']
        }

        current_type = result['doc_type'].lower()
        for normalized_type, variants in doc_type_mapping.items():
            if any(variant in current_type for variant in variants):
                result['doc_type'] = normalized_type
                break

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    result['extraction_confidence'] = _calculate_extraction_confidence(result)
    result['fields_extracted'] = len([k for k, v in result.items() if v and not k.startswith('_')])

    return result


def _normalize_date_string(date_str: str) -> Optional[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD"""
    if not date_str:
        return None

    # –†–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
    date_formats = [
        '%d.%m.%Y', '%d/%m/%Y', '%Y-%m-%d', '%d-%m-%Y',
        '%d.%m.%y', '%d/%m/%y', '%y-%m-%d'
    ]

    for fmt in date_formats:
        try:
            date_obj = datetime.strptime(date_str.strip(), fmt)
            return date_obj.strftime('%Y-%m-%d')
        except ValueError:
            continue

    return None


def _calculate_extraction_confidence(data: Dict) -> float:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π"""
    important_fields = ['fio', 'date', 'sum', 'phone', 'email']
    found_important = sum(1 for field in important_fields if field in data and data[field])

    total_fields = len([k for k, v in data.items() if v and not k.startswith('_')])

    # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    confidence = found_important / len(important_fields) * 0.7

    # –ë–æ–Ω—É—Å –∑–∞ –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π
    confidence += min(total_fields / 10, 0.3)

    return min(confidence, 1.0)


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏

def merge_extraction_results(ocr_result: Dict, llm_result: Dict) -> Dict[str, Any]:
    """
    –°–ª–∏—è–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ OCR –∏ LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏

    Args:
        ocr_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ OCR
        llm_result: –†–µ–∑—É–ª—å—Ç–∞—Ç LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏

    Returns:
        –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º LLM
    """
    merged = ocr_result.copy() if ocr_result else {}

    if llm_result:
        # LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–º–µ—é—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–ª–µ–π
        priority_fields = ['fio', 'date', 'sum', 'doc_type', 'phone', 'email']

        for field in priority_fields:
            if field in llm_result and llm_result[field]:
                merged[field] = llm_result[field]
                merged[f'{field}_source'] = 'llm'
            elif field in merged and merged[field]:
                merged[f'{field}_source'] = 'ocr'

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑ LLM
        for field, value in llm_result.items():
            if field not in merged and value:
                merged[field] = value
                merged[f'{field}_source'] = 'llm'

    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Å–ª–∏—è–Ω–∏–∏
    merged['merged'] = True
    merged['merge_timestamp'] = datetime.now().isoformat()

    return merged


def validate_extraction_quality(result: Dict) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö

    Args:
        result: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ—Ü–µ–Ω–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    quality_report = {
        'overall_quality': 'unknown',
        'field_quality': {},
        'recommendations': []
    }

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    field_checks = {
        'fio': lambda x: len(x.split()) >= 2 if isinstance(x, str) else False,
        'phone': lambda x: len(re.sub(r'[^\d]', '', str(x))) >= 10,
        'email': lambda x: '@' in str(x) and '.' in str(x),
        'inn': lambda x: len(re.sub(r'[^\d]', '', str(x))) in [10, 12],
        'date': lambda x: re.match(r'^\d{4}-\d{2}-\d{2}$', str(x)) is not None
    }

    total_score = 0
    checked_fields = 0

    for field, check_func in field_checks.items():
        if field in result and result[field]:
            is_valid = check_func(result[field])
            quality_report['field_quality'][field] = 'good' if is_valid else 'poor'

            if is_valid:
                total_score += 1
            else:
                quality_report['recommendations'].append(f'–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–ª–µ {field}: {result[field]}')

            checked_fields += 1

    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
    if checked_fields == 0:
        quality_report['overall_quality'] = 'no_data'
    elif total_score / checked_fields >= 0.8:
        quality_report['overall_quality'] = 'excellent'
    elif total_score / checked_fields >= 0.6:
        quality_report['overall_quality'] = 'good'
    elif total_score / checked_fields >= 0.4:
        quality_report['overall_quality'] = 'fair'
    else:
        quality_report['overall_quality'] = 'poor'
        quality_report['recommendations'].append('–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö')

    quality_report['score'] = total_score / max(checked_fields, 1)

    return quality_report


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    test_text = """
    –î–æ–≥–æ–≤–æ—Ä ‚Ññ 123/2024
    –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á
    –î–∞—Ç–∞: 15.03.2024
    –°—É–º–º–∞: 50 000 —Ä—É–±.
    –¢–µ–ª–µ—Ñ–æ–Ω: +7 (495) 123-45-67
    Email: ivan@example.com
    –ò–ù–ù: 1234567890
    """

    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏")

    # –¢–µ—Å—Ç rules-based –æ–±—Ä–∞–±–æ—Ç–∫–∏
    print("\n1. Rules-based –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞:")
    result = rules_based_postprocess({}, test_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))

    # –¢–µ—Å—Ç LLM –æ–±—Ä–∞–±–æ—Ç–∫–∏ (mock)
    print("\n2. Mock LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞:")
    llm_result = call_llm_to_json(test_text, model_type='mock')
    print(json.dumps(llm_result, ensure_ascii=False, indent=2))

    # –¢–µ—Å—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    print("\n3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞:")
    quality = validate_extraction_quality(result)
    print(json.dumps(quality, ensure_ascii=False, indent=2))